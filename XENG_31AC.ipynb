{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGLISH R1A: Chinatown and the Culture of Exclusion\n",
    "**Instructor: Amy Lee**\n",
    "\n",
    "**Developers: Michaela Palmer, Maya Shen, Cynthia Leu, Chris Cheung**\n",
    "\n",
    "**FPF 2017**\n",
    "\n",
    "Welcome to lab! Please read this lab in its entirety, as the analysis will make a lot more sense with the background context provided.\n",
    "This lab is intended to be a hands-on introduction to data science as it can be applied to Chinatown demographics and analyzing primary texts.\n",
    "\n",
    "The goal is to understand the evolution of Chinatowns due to outside forces such as globalization, immigration, and urbanization.\n",
    "\n",
    "## What this lab will cover\n",
    "* Running Jupyter Notebooks\n",
    "* Data Analysis of Chinatowns' demographics\n",
    "* Visualization & Interpretation\n",
    "\n",
    "## What you need to do\n",
    "* Read the content, complete the questions\n",
    "* Analyze the data\n",
    "* Submit the assignment\n",
    "\n",
    "\n",
    "# 1. Running Jupyter Notebooks\n",
    "\n",
    "You are currently working in a Jupyter Notebook. A Notebook allows text and code to be combined into one document. Each rectangular section of a notebook is called a \"cell.\" There are two types of cells in this notebook: text cells and code cells. \n",
    "\n",
    "Jupyter allows you to run simulations and regressions in real time. To do this, select a code cell, and click the \"run cell\" button at the top that looks like ▶| to confirm any changes. Alternatively, you can hold down the `shift` key and then press `return` or `enter`.\n",
    "\n",
    "In the following simulations, anytime you see `In [ ]` you should click the \"run cell\" button to see output. **If you get an error message after running a cell, go back to the beginning of the lab and make sure that every previous code cell has been run.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Introduction to Python and Jupyter Notebooks: <a id='jupyter'></a>\n",
    "\n",
    "## 1. Cells, Arithmetic, and Code\n",
    "In a notebook, each rectangle containing text or code is called a *cell*.\n",
    "\n",
    "Cells (like this one) can be edited by double-clicking on them. This cell is a text cell, written in a simple format called [Markdown](http://daringfireball.net/projects/markdown/syntax) to add formatting and section headings.  You don't need to worry about Markdown today, but it's a pretty fun+easy tool to learn.\n",
    "\n",
    "After you edit a cell, click the \"run cell\" button at the top that looks like ▶| to confirm any changes. (Try not to delete the instructions.) You can also press `SHIFT-ENTER` to run any cell or progress from one cell to the next.\n",
    "\n",
    "Other cells contain code in the Python programming language.  Running a code cell will execute all of the code it contains.\n",
    "\n",
    "Try running this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now quickly go through some very basic functionality of Python, which we'll be using throughout the rest of this notebook.\n",
    "\n",
    "### 1.1 Arithmetic\n",
    "Quantitative information arises everywhere in data science. In addition to representing commands to `print` out lines, expressions can represent numbers and methods of combining numbers. \n",
    "\n",
    "The expression `3.2500` evaluates to the number 3.25. (Run the cell and see.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't necessarily always need to say \"`print`\", because Jupyter always prints the last line in a code cell. If you want to print more than one line, though, do specify \"`print`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3)\n",
    "4\n",
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many basic arithmetic operations are built in to Python, like `*` (multiplication), `+` (addition), `-` (subtraction), and `/` (division). There are many others, which you can find information about [here](http://www.inferentialthinking.com/chapters/03/1/expressions.html). Use parenthesis to specify the order of operations, which act according to PEMDAS, just as you may have learned in school. Use parentheses for a happy new year!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + (6 * 5 - (6 * 3)) ** 2 * (( 2 ** 3 ) / 4 * 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Variables\n",
    "\n",
    "We sometimes want to work with the result of some computation more than once. To be able to do that without repeating code everywhere we want to use it, we can store it in a variable with *assignment statements*, which have the variable name on the left, an equals sign, and the expression to be evaluated and stored on the right. In the cell below, `(3 * 11 + 5) / 2 - 9` evaluates to 10, and gets stored in the variable `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = (3 * 11 + 5) / 2 - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions\n",
    "\n",
    "One important form of an expression is the call expression, which first names a function and then describes its arguments. The function returns some value, based on its arguments. Some important mathematical functions are:\n",
    "\n",
    "| Function | Description                                                   |\n",
    "|----------|---------------------------------------------------------------|\n",
    "| `abs`      | Returns the absolute value of its argument                    |\n",
    "| `max`      | Returns the maximum of all its arguments                      |\n",
    "| `min`      | Returns the minimum of all its arguments                      |\n",
    "| `round`    | Round its argument to the nearest integer                     |\n",
    "\n",
    "Here are two call expressions that both evaluate to 3\n",
    "\n",
    "```python\n",
    "abs(2 - 5)\n",
    "max(round(2.8), min(pow(2, 10), -1 * pow(2, 10)))\n",
    "```\n",
    "\n",
    "These function calls first evaluate the expressions in the arguments (inside the parentheses), then evaluate the function on the results. `abs(2-5)` evaluates first to `abs(3)`, then returns `3`.\n",
    "\n",
    "A **statement** is a whole line of code.  Some statements are just expressions, like the examples above, that can be broken down into its subexpressions which get evaluated individually before evaluating the statement as a whole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calling functions\n",
    "\n",
    "The most common way to combine or manipulate values in Python is by calling functions. Python comes with many built-in functions that perform common operations.\n",
    "\n",
    "For example, the `abs` function takes a single number as its argument and returns the absolute value of that number.  The absolute value of a number is its distance from 0 on the number line, so `abs(5)` is 5 and `abs(-5)` is also 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions can be called as above, putting the argument in parentheses at the end, or by using \"dot notation\", and calling the function after finding the arguments, as in the cell immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [1, 2, 5]  # a list of items, in this case, numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.reverse()  # reverses the item order\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploring Demographic Data: <a id='jupyter'></a>\n",
    "\n",
    "## 1.1 Importing Modules\n",
    "\n",
    "First, we need to import libraries so that we are able to call the functions from within. We are going to use these functions to manipulate data tables and conduct math operations in our analysis. Run the code cell below to import these modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "%matplotlib inline\n",
    "# !pip install --no-cache-dir -U -q folium\n",
    "# import folium\n",
    "# import pandas as pd\n",
    "# from IPython.display import HTML, display, IFrame\n",
    "# from folium import plugins\n",
    "import spacy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to work with tables and explore some real data. A Table is just like how we made a list above with make_array, but for all the rows in a table.\n",
    "<p>First, let's load in all our of decennial San Francisco Chinatown census data acquired from an online domain called Social Explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in all of the data\n",
    "sf_1940 = Table.read_table('data/sf_1940.csv')\n",
    "sf_1950 = Table.read_table('data/sf_1950.csv')\n",
    "sf_1960 = Table.read_table('data/sf_1960.csv')\n",
    "sf_1970 = Table.read_table('data/sf_1970.csv')\n",
    "sf_1980 = Table.read_table('data/sf_1980.csv')\n",
    "sf_1990 = Table.read_table('data/sf_1990.csv')\n",
    "sf_2000 = Table.read_table('data/sf_2000.csv')\n",
    "sf_2010 = Table.read_table('data/sf_2010.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these tables by running this block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That's a lot of data and a lot of data that we don't need. In each dataset there are a lot of columns with datasets that we aren't interested in. We need to extract the valuable pieces of data from each year as well as omit the rows that don't provide us with useful information. Since we are analyzing the changing demographics of Chinatown, we need to extract the population data by selecting the relevant columns from each table. Analyzing data from the U.S. Census can be a little tricky because each year the datasets they acquire and the survey questions they ask are a little different. As a result, some years we get a number as specific as the population of Chinese in a given tract, and in other years the most specific information available to us is the \"Nonwhite\" population. We have to work with what we have to get an accurate as possible idea of the population of the Chinese in Chinatown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_1940_pop = sf_1940.select('Full Name', 'Total Population', 'White', 'Black', 'Other')\n",
    "sf_1940_pop = sf_1940_pop.take(range(1, sf_1940_pop.num_rows))\n",
    "sf_1940_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this table is a lot more readable, with the specific columns that we want. Now that you have an idea of how to clean the data, we'll do the rest for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_1950_pop = sf_1950.select('Full Name', 'Total Population', 'White', 'Black', 'Other')\n",
    "sf_1950_pop = sf_1950_pop.take(range(1, sf_1950_pop.num_rows))\n",
    "sf_1960_pop = sf_1960.select('Qualified geography name', 'Total Population', 'Total Population: White', \n",
    "                             'Total Population: Black', 'Total Population: Other Race')\n",
    "sf_1960_pop = sf_1960_pop.take(range(1, sf_1960_pop.num_rows))\n",
    "sf_1970_pop = sf_1970.select('Qualifying Name', 'Total Population', 'White', 'Black', 'Some Other Race', \n",
    "                             'Count of Persons of Foreign Stock',\n",
    "                             'Count of Persons of Foreign Stock: Native (of foreign or mixed parentage)', \n",
    "                             'Count of Persons of Foreign Stock: Native (of foreign or mixed parentage): China', \n",
    "                             'Count of Persons of Foreign Stock: Foreign born', \n",
    "                             'Count of Persons of Foreign Stock: Foreign born: China')\n",
    "sf_1970_pop = sf_1970_pop.take(range(1, sf_1970_pop.num_rows))\n",
    "sf_1980_pop = sf_1980.select('Qualifying Name', 'Total Population', 'Total Population: White', \n",
    "                             'Total Population: Black', 'Total Population: Asian and Pacific Islander')\n",
    "sf_1980_pop = sf_1980_pop.take(range(1, sf_1980_pop.num_rows))\n",
    "sf_1990_pop = sf_1990.select('Qualifying Name', 'Total Population', 'Persons: White', \n",
    "                             'Persons: Black', 'Persons: Asian or Pacific Islander')\n",
    "sf_1990_pop = sf_1990_pop.take(range(1, sf_1990_pop.num_rows))\n",
    "sf_2000_pop = sf_2000.select('Qualifying Name', 'Total Population', 'White Alone', 'Black or African American Alone', \n",
    "                             'Asian Alone', 'Chinese, except Taiwanese')\n",
    "sf_2000_pop = sf_2000_pop.take(range(1, sf_2000_pop.num_rows))\n",
    "sf_2010_pop = sf_2010.select('Qualifying Name', 'Total Population', 'Total population: White alone', \n",
    "                             'Total population: Black or African American alone', 'Total population: Asian alone', \n",
    "                             'Total Asian with one Asian category only: Chinese (except Taiwanese)')\n",
    "sf_2010_pop = sf_2010_pop.take(range(1, sf_2010_pop.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mh_1960 = Table.read_table('data/manhattan_1960.csv')\n",
    "mh_1990 = Table.read_table('data/manhattan_1990.csv')\n",
    "mh_2000 = Table.read_table('data/manhattan_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mh_1960_pop = mh_1960.select('Geo_QName', 'SE_T001_001', 'SE_T014_001', 'SE_T014_002').relabeled(['Geo_QName', \n",
    "                            'SE_T001_001', 'SE_T014_001', 'SE_T014_002'], ['Qualifying Name', 'Total Population', \n",
    "                            'White', 'Nonwhite'])\n",
    "mh_1960_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mh_1990_pop = mh_1990.select('Geo_QName', 'SE_T005_001', 'SE_T012_002', 'SE_T012_005', \n",
    "                             'SE_T015_002').relabeled(['Geo_QName', 'SE_T005_001', 'SE_T012_002', \n",
    "                            'SE_T012_005', 'SE_T015_002'], ['Qualifying Name', 'Total Population', \n",
    "                            'White', 'Asian or Pacific Islander', 'Chinese'])\n",
    "mh_1990_pop = mh_1990_pop.take(range(0, mh_1990_pop.num_rows))\n",
    "mh_1990_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mh_2000_pop = mh_2000.select('Geo_QName', 'SE_T005_001', 'SE_T014_002', 'SE_T014_005', \n",
    "                             'SE_T019_005').relabeled(['Geo_QName', 'SE_T005_001', 'SE_T014_002', 'SE_T014_005', \n",
    "                             'SE_T019_005'], ['Qualifying Name', 'Total Population', \n",
    "                            'White Alone', 'Asian Alone', 'Chinese'])\n",
    "mh_2000_pop = mh_2000_pop.take(range(0, mh_2000_pop.num_rows))\n",
    "mh_2000_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = Table.read_table('data/Boston Chinatown Population.csv')\n",
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analyzing Demographics\n",
    "In this section, we will examine some of the factors that influence population growth and how they are changing the landscape of Chinatowns across the U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SF_COORDINATES = (37.79937840, -122.40638790) # create empty map zoomed in AOI\n",
    "map = folium.Map(location=SF_COORDINATES, zoom_start=13).save(\"map.html\")\n",
    "IFrame('map.html', width=700, height=400)\n",
    "\n",
    "# need to bind pop data to boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first examine the Asian percent population change over the years in Chinatown. To do this, we have to first aggregate all of the Census tract data from each year onto a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_total_pop = Table().with_columns(\n",
    "    'Year', make_array(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010),\n",
    "    'Total Population', make_array(sum([int(i) for i in sf_1940_pop.column(1)]), sum([int(i) for i in sf_1950_pop.column(1)]), sum([int(i) for i in sf_1960_pop.column(1)]), sum([int(i) for i in sf_1970_pop.column(1)]), sum([int(i) for i in sf_1980_pop.column(1)]), sum([int(i) for i in sf_1990_pop.column(1)]), sum([int(i) for i in sf_2000_pop.column(1)]), sum([int(i) for i in sf_2010_pop.column(1)]))\n",
    "    \n",
    ")\n",
    "sf_total_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MANHATTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reading Primary Texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to further analyze some of the poems we have been discussing written by Chinese immigrants on Angel Island. \n",
    "<p>Run the following cell to import the poems from a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/islandpoetry1_22.txt', \"r\") as f:\n",
    "    raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in which words appear the most often in our set of poems. It's pretty hard to read or see much in this form. We'll coming back to the topic of what words are the most common with actual numbers a bit later but for now, run the following cell to generate two interesting visualizations of the most common words (minus those such as \"the\", \"a\", etc.). hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(raw)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(raw)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it seems we've forgotten just how many poems we have in our set. Luckily we have a quick way of finding out! Each \"\\n\" in the display of the poem text indicates a line break. It turns out that each poem is separated by an empty line, a.k.a. two line breaks or \"\\n\"'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_poems = len(raw.split(\"\\n\\n\"))\n",
    "num_poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also use this idea to calculate the number of characters in each poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_char_per_poem = [len(p) for p in raw.split(\"\\n\\n\")]\n",
    "print(num_char_per_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting but seems like just a long list of numbers. What about the average number of characters per poem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_char = np.mean(num_char_per_poem)\n",
    "avg_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use \"\\n\" to look at enjambment. Let's calculate the proportion of lines that are enjambed out of the total number of lines per poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "poems = raw.split(\"\\n\\n\")\n",
    "\n",
    "all_poems_enjambment = []\n",
    "for p in poems:\n",
    "    lines = p.split(\"\\n\")\n",
    "    enjambment = 0\n",
    "    for l in lines:\n",
    "        try:\n",
    "            if l[-1] in punctuation:\n",
    "                pass\n",
    "            else:\n",
    "                enjambment += 1\n",
    "        except:\n",
    "            pass\n",
    "    enj = enjambment/len(lines)\n",
    "    all_poems_enjambment.append(enj)\n",
    "\n",
    "print(all_poems_enjambment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, what about the average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_poems_enjambment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now return to the question of the words that appear the most frequently in these 49 poems. First we have to use spaCy, an open-source software library for Natural Language Processing (NLP), to parse through the text and replace all the \"\\n\"'s with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', parser=False)\n",
    "parsed_text = nlp(raw.replace(\"\\n\", \" \"))\n",
    "parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate all the words/symbols and put them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab = Table()\n",
    "toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new table with even more columns using the \"tablefy\" function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tablefy(parsed_text):\n",
    "    toks_tab = Table()\n",
    "    toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Lemma\", values=[word.lemma_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Stop Word\", values=[word.is_stop for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Punctuation\", values=[word.is_punct for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Space\", values=[word.is_space for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Number\", values=[word.like_num for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"OOV\", values=[word.is_oov for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Dependency\", values=[word.dep_ for word in parsed_text])\n",
    "    return toks_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablefy(parsed_text).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the frequency of words. However, we want to get rid of words such as \"the\" and \"and\" (stop words), punctuation, and spaces. We can do this by selecting rows that are not stop words, punctuation, or spaces and then sorting by word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = tablefy(parsed_text).where(\"Stop Word\", are.equal_to(False)).where(\n",
    "    \"Punctuation\", are.equal_to(False)).where(\n",
    "    \"Space\", are.equal_to(False)).group(\"Word\").sort(\"count\",descending=True)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see the words \"sad\" and \"sadness\" - it seems strange to separate them. It turns out that these words are part of the same \"lexeme\", or a unit of meaning. For example, \"run\", \"runs\", \"ran\", and \"running\" are all part of the same lexeme with the lemma sad. Lemmas are another column in our above table! Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_counts = tablefy(parsed_text).where(\"Stop Word\", are.equal_to(False)).where(\n",
    "    \"Punctuation\", are.equal_to(False)).where(\n",
    "    \"Space\", are.equal_to(False)).group(\"Lemma\").sort(\"count\",descending=True)\n",
    "lemma_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how many words there are of each part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = tablefy(parsed_text).where(\"Stop Word\", are.equal_to(False)).where(\n",
    "    \"Punctuation\", are.equal_to(False)).where(\n",
    "    \"Space\", are.equal_to(False)).group(\"POS\").sort(\"count\",descending=True)\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the proportions of them out of all the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(pos_counts.num_rows):\n",
    "    pos = pos_counts.column(\"POS\").item(i)\n",
    "    count = pos_counts.column(\"count\").item(i)\n",
    "    print(count)\n",
    "    total = word_counts.num_rows\n",
    "    print(total)\n",
    "    proportion = str(count / total)\n",
    "    print(pos + \" proportion: \" + proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing some exploring of your own! If you're feeling stuck, try copying and editing code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your own code here!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
